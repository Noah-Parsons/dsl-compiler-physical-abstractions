# -*- coding: utf-8 -*-
"""DSL Compiler for Physical Abstractions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sgq21EAe9ogpBLgjPWZs2kuYx0ppLJoe
"""

# === Phase 1.1: DSL Compiler for Physical Abstractions ===

import re                              # Import the regular expressions module for string pattern matching and manipulation
from typing import List, Union, Tuple  # Import type hints: List for arrays, Union for multiple possible types, Tuple for fixed-length sequences

# Define token types with regex patterns
TOKEN_TYPES = [                           # List of token types, each as a tuple (name, regex pattern)
    ("COMMAND", r"\\[a-zA-Z]+"),          # LaTeX-style command (e.g., \alpha, \frac)
    ("LBRACE", r"\{"),                    # Left curly brace '{'
    ("RBRACE", r"\}"),                    # Right curly brace '}'
    ("LPAREN", r"\("),                    # Left parenthesis '('
    ("RPAREN", r"\)"),                    # Right parenthesis ')'
    ("OPERATOR", r"="),                   # Equals sign '=' (assignment or equality)
    ("COMMA", r","),                      # Comma separator ','
    ("NUMBER", r"\d+(\.\d+)?"),           # Integer or floating-point number (e.g., 42, 3.14)
    ("IDENT", r"[a-zA-Z_][a-zA-Z0-9_]*"), # Identifier (variable/function name)
    ("WHITESPACE", r"\s+"),               # Whitespace (used for skipping spaces/tabs)
]

# Combine into one regex with named groups
token_regex = "|".join(f"(?P<{name}>{pattern})" for name, pattern in TOKEN_TYPES) # Build named groups for each token type
token_pattern = re.compile(token_regex)                                           # Compile the combined regular expression into a pattern object


# Token class
class Token:
    def __init__(self, type_: str, value: str): # Initialize a Token with a type and its matched string
        self.type = type_                       # Token type (e.g., 'NUMBER', 'COMMAND')
        self.value = value                      # Actual matched text from the source

    def __repr__(self):                    # String representation for debugging and printing
        return f"{self.type}:{self.value}" # Return token in 'TYPE:value' format


# Tokenizer function that converts input string into a list of Token objects
def tokenize(source: str) -> List[Token]:        # Takes source code as input and returns a list of Token instances
    tokens = []                                  # Initialize empty list to hold tokens
    for match in token_pattern.finditer(source): # Iterate over all regex matches in the input string
        kind = match.lastgroup                   # Get the name of the matched group (i.e., token type)
        value = match.group()                    # Get the actual matched substring
        if kind != "WHITESPACE":                 # Skip whitespace tokens (do not add to result)
            tokens.append(Token(kind, value))    # Create Token instance and add to list
    return tokens                                # Return the final list of tokens

# AST node base class

class ASTNode:
    pass   # Base class for all Abstract Syntax Tree (AST) nodes (used for type hierarchy)

# Variable definition node: corresponds to \defvar{name}{type}{unit}
class VarDef(ASTNode):
    def __init__(self, name, vartype, unit):
        self.name = name                      # Variable name (e.g., "velocity")
        self.vartype = vartype                # Type of variable (e.g., "float", "int")
        self.unit = unit                      # Unit of measurement (e.g., "m/s", "kg")

    def __repr__(self):
        return f"VarDef(name='{self.name}', vartype='{self.vartype}', unit='{self.unit}')"  # String representation

# Operation definition node: corresponds to \define{ \op{name}(arg) = rhs }
class Op(ASTNode):
    def __init__(self, name, args):
        self.name = name            # Operator or function name (e.g., "f", "force")
        self.args = args            # List of arguments (e.g., ["mass", "acceleration"])

    def __repr__(self):
        return f"Op(name='{self.name}', args={self.args})"  # String representation

# Full define statement: maps an Op to a right-hand-side expression or string
class Define(ASTNode):
    def __init__(self, lhs, rhs):
        self.lhs = lhs            # Left-hand side is an Op instance representing the defined function/operator
        self.rhs = rhs            # Right-hand side is a string or parsed expression (e.g., "m*a")

    def __repr__(self):
        return f"Define(lhs={self.lhs}, rhs={self.rhs})"  # String representation

# Boundary condition node: corresponds to \boundary{expr}
class Boundary(ASTNode):
    def __init__(self, expr):
        self.expr = expr  # Boundary condition expression (e.g., "v=0 at t=0")

    def __repr__(self):
        return f"Boundary(expr='{self.expr}')"  # String representation

# Symmetry or invariant condition node: corresponds to \symmetry{law \invariant variable}
class Symmetry(ASTNode):
    def __init__(self, law, invariant):
        self.law = law                  # The symmetry law being applied (e.g., "Noether's Theorem")
        self.invariant = invariant      # The invariant quantity (e.g., "momentum")

    def __repr__(self):
        return f"Symmetry(law='{self.law}', invariant='{self.invariant}')"  # String representation

# Parser class: converts a flat list of tokens into a structured AST (abstract syntax tree)
class Parser:
    def __init__(self, tokens):
        # Tokens: list of Token objects produced by the lexer/tokenizer
        self.tokens = tokens
        self.pos = 0  # Current position (index) in the token list

    # Return the current token without consuming it
    def peek(self):
        if self.pos < len(self.tokens):
            return self.tokens[self.pos]
        return None  # No more tokens to process

    # Try to match and consume a token of a specific type
    def match(self, expected_type):
        token = self.peek()
        if token and token.type == expected_type:
            self.pos += 1  # Advance the token stream
            return token
        return None  # Token didn't match expected type

    # Require that the next token matches a given type or raise an error
    def expect(self, expected_type):
        token = self.match(expected_type)
        if not token:
            # Raise syntax error if the expected token isn't found
            raise SyntaxError(f"Expected {expected_type} but got {self.peek().type if self.peek() else 'EOF'}")
        return token

    # Main entry point for the parser: processes all top-level nodes
    def parse(self):
        nodes = []                          # List of parsed AST nodes (VarDef, Define, etc.)
        while self.pos < len(self.tokens):  # Continue until all tokens consumed
            token = self.peek()
            if token.type == "COMMAND":
                # Dispatch to the appropriate handler based on the command name
                if token.value == r"\defvar":
                    nodes.append(self.parse_defvar())
                elif token.value == r"\define":
                    nodes.append(self.parse_define())
                elif token.value == r"\boundary":
                    nodes.append(self.parse_boundary())
                elif token.value == r"\symmetry":
                    nodes.append(self.parse_symmetry())
                else:
                    raise SyntaxError(f"Unknown command: {token.value}")
            else:
                # Skip over non-command tokens (e.g. whitespace, comments)
                self.pos += 1
        return nodes  # Return the full AST

    # Parse \defvar{name}{type}{unit} into a VarDef node
    def parse_defvar(self):
        self.expect("COMMAND")                # Consume \defvar
        self.expect("LBRACE")                 # Start of {name}
        name = self.expect("IDENT").value
        self.expect("RBRACE")                 # End of {name}
        self.expect("LBRACE")                 # Start of {type}
        vartype = self.expect("IDENT").value
        self.expect("RBRACE")                 # End of {type}
        self.expect("LBRACE")                 # Start of {unit}
        unit = self.expect("IDENT").value
        self.expect("RBRACE")                 # End of {unit}
        return VarDef(name, vartype, unit)    # Create and return AST node

    # Parse \define{\op{name}(arg) = rhs} into a Define(Op(...), rhs) node
    def parse_define(self):
        self.expect("COMMAND")                # Consume \define
        self.expect("LBRACE")                 # Start of full definition

        self.expect("COMMAND")                # Consume \op
        self.expect("LBRACE")                 # Start of {name}
        op_name = self.expect("IDENT").value
        self.expect("RBRACE")                 # End of {name}

        self.expect("LPAREN")                 # Consume (
        arg = self.expect("IDENT").value
        self.expect("RPAREN")                 # Consume )

        self.expect("OPERATOR")               # Expect = sign

        # Right-hand side: either a NUMBER or IDENT (can be extended to expressions)
        rhs_token = self.expect("NUMBER") or self.expect("IDENT")
        rhs = rhs_token.value

        self.expect("RBRACE")        # End of definition

        return Define(Op(op_name, [arg]), rhs)  # return Define node with Op(lhs) and RHS

    # Parse \boundary{expr} into a Boundary node
    def parse_boundary(self):
        self.expect("COMMAND")              # Consume \boundary
        self.expect("LBRACE")               # Start of expression
        expr = self.expect("IDENT").value
        self.expect("RBRACE")               # End of expression
        return Boundary(expr)               # Return Boundary node

    # Parse \symmetry{law \invariant variable} into a Symmetry node
    def parse_symmetry(self):
        self.expect("COMMAND")              # Consume \symmetry
        self.expect("LBRACE")               # Start of content

        law = self.expect("IDENT").value  # Symmetry law name

        self.expect("COMMAND")                  # Consume \invariant keyword
        invariant = self.expect("IDENT").value  # The invariant variable

        self.expect("RBRACE")  # End of content

        return Symmetry(law, invariant)  # Return Symmetry node

# TypeChecker class: performs semantic analysis on the AST by validating type consistency, defined variables, and structure of expressions.
class TypeChecker:
    def __init__(self):
        # Dictionary to store defined variables:
        # Keys = variable names, values = (type, unit)
        self.variables = {}

    # Main type-checking entry point: receives AST from parser
    def check(self, ast):
        for node in ast:  # Iterate through all top-level nodes
            if isinstance(node, VarDef):
                # Register variable with its type and unit
                self.variables[node.name] = (node.vartype, node.unit)
                print(f"[TypeCheck] {node.name}: type = {node.vartype}, unit = {node.unit}")

            elif isinstance(node, Define):
                # Check that all arguments in the operator are defined variables
                if isinstance(node.lhs, Op):
                    for arg in node.lhs.args:
                        if arg not in self.variables:
                            raise TypeError(f"Undefined variable in operator: {arg}")
                # Optional: check if RHS is a variable and whether units/types match
                if isinstance(node.rhs, str) and node.rhs in self.variables:
                    # Currently no unit/type enforcement between lhs and rhs
                    pass  # Could insert additional consistency checks here

            elif isinstance(node, Boundary):
                # Ensure the boundary variable has been defined
                if node.expr not in self.variables:
                    raise TypeError(f"Boundary references undefined variable: {node.expr}")
                print(f"[TypeCheck] Boundary condition on: {node.expr}")

            elif isinstance(node, Symmetry):
                # Only logging currently; does not enforce law/invariant semantics
                print(f"[TypeCheck] Symmetry declared: {node.law} invariant under {node.invariant}")

# IRCompiler: Translates the AST into a simple category-theoretic IR.
# Objects represent variables; morphisms represent transformations, laws, boundaries, or symmetries.
class IRCompiler:
    def __init__(self):
        # Dictionary of defined objects (variables), mapping names to type/unit info
        self.objects = {}

        # List of morphisms: each is a tuple (name, domain, codomain, description)
        self.morphisms = []

    # Main method to compile a list of AST nodes into categorical IR
    def compile(self, ast):
        print("\n[IR] Compiling to Categorical IR...")
        for node in ast:
            if isinstance(node, VarDef):
                # Register a variable as an object in the category
                self.objects[node.name] = {
                    "type": node.vartype,
                    "unit": node.unit
                }

            elif isinstance(node, Define):
                # Encode a definition as a morphism from arg -> result
                if isinstance(node.lhs, Op):
                    morphism_name = f"define_{node.lhs.name}"  # Unique identifier
                    domain = node.lhs.args[0] if node.lhs.args else "Unknown"  # Source object
                    codomain = domain  # Assumes codomain is same for now (could refine later)
                    law_desc = f"{node.lhs.name}({', '.join(node.lhs.args)}) = {node.rhs}"  # Descriptive label
                    self.morphisms.append((morphism_name, domain, codomain, law_desc))

            elif isinstance(node, Boundary):
                # Treat a boundary condition as a morphism from the variable to "None"
                var = node.expr
                self.morphisms.append((f"boundary_{var}", var, "None", "Boundary condition"))

            elif isinstance(node, Symmetry):
                # Encode a symmetry or invariant law as a morphism with no domain/codomain
                self.morphisms.append((
                    f"symmetry_{node.law}",
                    "None",
                    "None",
                    f"Invariant under {node.invariant}"
                ))

        # Output the IR in readable form
        self.pretty_print()

    # Helper method to print the IR in human-readable format
    def pretty_print(self):
        print("\n[IR] Objects:")
        for obj, props in self.objects.items():
            print(f" - {obj} : {props['type']} [{props['unit']}]")

        print("\n[IR] Morphisms:")
        for name, domain, codomain, desc in self.morphisms:
            print(f" - {name}: {domain} -> {codomain} | {desc}")

# === Run the DSL Compiler ===

# Step 1: Define the source program using the custom DSL syntax
dsl = r"""
\defvar{T}{Real}{kelvin}
\define{ \op{laplace}(T) = 0 }
\boundary{T}
\symmetry{L \invariant t}
"""

# Step 2: Lexical Analysis - Convert the raw DSL string into tokens
tokens = tokenize(dsl)

# Step 3: Parsing - Convert the token stream into an Abstract Syntax Tree (AST)
parser = Parser(tokens)
ast = parser.parse()

# Step 4: Show the AST to inspect parsed nodes
print("\n[AST]")
for stmt in ast:
    print(f" - {stmt}")  # Displays each parsed statement node

# Step 5: Static Type Checking - Ensure all variables used are declared and consistent
checker = TypeChecker()
checker.check(ast)

# Step 6: IR Compilation - Convert AST into a categorical intermediate representation
compiler = IRCompiler()
compiler.compile(ast)